{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:\n",
    "\n",
    "Student ID: 114062635\n",
    "\n",
    "GitHub ID: Benson920613\n",
    "\n",
    "Kaggle name: 游睿騏\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "#### 1.1.1 Data Loading and Merging\n",
    "\n",
    "First load the raw JSON posts and the two CSV files (`emotion.csv`, `data_identification.csv`), then normalize and join them:\n",
    "\n",
    "```python\n",
    "def load_data(data_dir):\n",
    "    with open(\"final_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    posts = pd.json_normalize(data).rename(\n",
    "        columns={\n",
    "            \"root._source.post.post_id\": \"post_id\",\n",
    "            \"root._source.post.text\": \"text\",\n",
    "            \"root._source.post.hashtags\": \"hashtags\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def merge_hashtags(row):\n",
    "        text = str(row[\"text\"])\n",
    "        if isinstance(row[\"hashtags\"], list) and len(row[\"hashtags\"]) > 0:\n",
    "            tags = \" \".join(row[\"hashtags\"])\n",
    "            return f\"{text} {tags}\"\n",
    "        return text\n",
    "\n",
    "    posts[\"text\"] = posts.apply(merge_hashtags, axis=1)\n",
    "\n",
    "    labels = pd.read_csv(\"emotion.csv\").rename(columns={\"id\": \"post_id\"})\n",
    "    split = pd.read_csv(\"data_identification.csv\").rename(columns={\"id\": \"post_id\"})\n",
    "\n",
    "    df = posts.merge(split, on=\"post_id\", how=\"left\").merge(labels, on=\"post_id\", how=\"left\")\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Highlights**:\n",
    "\n",
    "* Hashtags are **merged into the text** to preserve their emotional information.\n",
    "* The merged DataFrame contains: `post_id`, `text` (with hashtags appended), `emotion`, and `split` (train/test).\n",
    "\n",
    "#### 1.1.2 Text Cleaning\n",
    "\n",
    "Keeping the original casing (no `.lower()`), but remove noisy patterns that are not helpful for emotion:\n",
    "\n",
    "```python\n",
    "URL_PATTERN = re.compile(r\"http\\S+|www\\S+\")\n",
    "MENTION_PATTERN = re.compile(r\"@\\w+\")\n",
    "HTML_TAG_PATTERN = re.compile(r\"<.*?>\")\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # Keep original casing\n",
    "    text = URL_PATTERN.sub(\"\", text)       # remove URLs\n",
    "    text = MENTION_PATTERN.sub(\"\", text)   # remove @mentions\n",
    "    text = HTML_TAG_PATTERN.sub(\"\", text)  # remove HTML tags\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def add_clean_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"clean_text\"] = df[\"text\"].astype(str).apply(preprocess_text)\n",
    "    return df\n",
    "```\n",
    "\n",
    "Design key points:\n",
    "\n",
    "* **URLs, user mentions, and HTML** are removed as they rarely convey emotion by themselves.\n",
    "* **Case is preserved** to keep cues like all-caps or proper nouns.\n",
    "* Emojis and hashtags are kept (after merging) and handled by the tokenizer.\n",
    "\n",
    "#### 1.1.3 Dataset Construction and Tokenization\n",
    "\n",
    "Mapping labels to IDs, split train/validation, and build HuggingFace Datasets using `clean_text`:\n",
    "\n",
    "```python\n",
    "LABEL_ORDER = [\"joy\", \"anger\", \"surprise\", \"sadness\", \"fear\", \"disgust\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(LABEL_ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "def prepare_splits(df: pd.DataFrame, test_size: float = 0.1, seed: int = 42):\n",
    "    df = df.copy()\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_test = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "    df_train[\"label_id\"] = df_train[\"emotion\"].map(LABEL2ID)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_train,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "        stratify=df_train[\"label_id\"],\n",
    "    )\n",
    "\n",
    "    train_df = train_df.rename(columns={\"label_id\": \"labels\"})\n",
    "    val_df = val_df.rename(columns={\"label_id\": \"labels\"})\n",
    "\n",
    "    return train_df, val_df, df_test\n",
    "```\n",
    "\n",
    "Then tokenize with the RoBERTa-based emotion model:\n",
    "\n",
    "```python\n",
    "def build_datasets(train_df, val_df, test_df, model_name=\"cardiffnlp/twitter-roberta-base-emotion\", max_len=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(\n",
    "        train_df[[\"clean_text\", \"labels\"]].reset_index(drop=True)\n",
    "    )\n",
    "    val_ds = Dataset.from_pandas(\n",
    "        val_df[[\"clean_text\", \"labels\"]].reset_index(drop=True)\n",
    "    )\n",
    "    test_ds = Dataset.from_pandas(\n",
    "        test_df[[\"clean_text\"]].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"clean_text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "        )\n",
    "\n",
    "    train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "    val_ds = val_ds.map(tokenize_batch, batched=True)\n",
    "    test_ds = test_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "    # Remove helper columns and set PyTorch format\n",
    "    ...\n",
    "    return tokenizer, train_ds, val_ds, test_ds\n",
    "```\n",
    "\n",
    "* Maximum sequence length: **128**\n",
    "* Padding strategy: `padding=\"max_length\"`\n",
    "* Datasets are formatted as PyTorch tensors (`input_ids`, `attention_mask`, `labels` for train/val).\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "I did not add separate hand-crafted numeric features; instead, relying on RoBERTa’s contextual embeddings. The main “feature engineering” is:\n",
    "\n",
    "1. **Label Encoding**\n",
    "\n",
    "   * Map 6 emotions to IDs using `LABEL_ORDER`.\n",
    "   * This is required for HuggingFace models and for stratified splitting.\n",
    "\n",
    "2. **Class Weights (Implemented but Disabled in Final Model)**\n",
    "\n",
    "    Implemented class weight computation to counter label imbalance:\n",
    "\n",
    "   ```python\n",
    "   def compute_class_weights(df_train: pd.DataFrame) -> torch.Tensor:\n",
    "       counts = df_train[\"labels\"].value_counts().sort_index().values\n",
    "       total = counts.sum()\n",
    "       num_classes = len(counts)\n",
    "       weights = total / (num_classes * counts)\n",
    "       return torch.tensor(weights, dtype=torch.float32)\n",
    "   ```\n",
    "\n",
    "   This is passed into a custom `WeightedTrainer`:\n",
    "\n",
    "   ```python\n",
    "   class WeightedTrainer(Trainer):\n",
    "       def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n",
    "           super().__init__(*args, **kwargs)\n",
    "           self.class_weights = class_weights\n",
    "\n",
    "       def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "           labels = inputs.pop(\"labels\")\n",
    "           outputs = model(**inputs)\n",
    "           logits = outputs.logits\n",
    "           loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "           loss = loss_fct(logits, labels)\n",
    "           return (loss, outputs) if return_outputs else loss\n",
    "   ```\n",
    "\n",
    "   However, for the **final best-scoring submission**, I set:\n",
    "\n",
    "   ```python\n",
    "   USE_CLASS_WEIGHTS = False\n",
    "   ```\n",
    "\n",
    "   because enabling class weights did **not** improve the leaderboard score.\n",
    "\n",
    "So, the final “features” are purely the tokenized **`clean_text`** sequences, which RoBERTa transforms into rich embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "Our final model is based on the pretrained **RoBERTa emotion model**:\n",
    "\n",
    "* Model checkpoint: `cardiffnlp/twitter-roberta-base-emotion`\n",
    "* Architecture: RoBERTa-base encoder + classification head for 6 emotions\n",
    "* Number of labels: 6 (joy, anger, surprise, sadness, fear, disgust)\n",
    "\n",
    "#### 1.3.1 Model Initialization\n",
    "\n",
    "```python\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(LABEL_ORDER),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "```\n",
    "\n",
    "* `ignore_mismatched_sizes=True` is used to safely align the classification head with our 6 labels.\n",
    "\n",
    "#### 1.3.2 Training Configuration\n",
    "\n",
    "Using HuggingFace `TrainingArguments` and `Trainer`:\n",
    "\n",
    "```python\n",
    "batch_size = 32\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-emotion\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    ")\n",
    "```\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "* **Epochs**: 6\n",
    "* **Batch size**: 32\n",
    "* **Learning rate**: 2e-5\n",
    "* **Weight decay**: 0.01\n",
    "* **Selection metric**: `macro_f1`\n",
    "* **Best model** automatically reloaded at the end.\n",
    "\n",
    "#### 1.3.3 Trainer and Metrics\n",
    "\n",
    "Computed both accuracy and macro-F1:\n",
    "\n",
    "```python\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1_macro = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1_macro}\n",
    "```\n",
    "\n",
    "Final Trainer (without class weights):\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "```\n",
    "\n",
    "Train and then predict on the test set:\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "pred_output = trainer.predict(test_ds)\n",
    "test_preds_id = pred_output.predictions.argmax(axis=-1)\n",
    "test_preds_label = [ID2LABEL[int(i)] for i in test_preds_id]\n",
    "```\n",
    "\n",
    "The final submission is written to `submission_distilbert_weighted.csv` (with column renamed to `id`):\n",
    "\n",
    "```python\n",
    "submission = df[df[\"split\"] == \"test\"][[\"post_id\"]].copy()\n",
    "submission[\"emotion\"] = test_preds_label\n",
    "submission = submission.rename(columns={\"post_id\": \"id\"})\n",
    "submission.to_csv(out_path, index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "Even though the final best model is a relatively “simple” RoBERTa fine-tuning, I experimented with multiple variations:\n",
    "\n",
    "1. **Different Base Models**\n",
    "\n",
    "   * DistilBERT-based models.\n",
    "   * Different RoBERTa-based emotion checkpoint (`cardiffnlp/twitter-roberta-base-emotion`).\n",
    "\n",
    "   → The RoBERTa emotion model consistently gave better scores on the scoring.\n",
    "\n",
    "2. **Class Weights ON vs OFF**\n",
    "\n",
    "   * Implemented `compute_class_weights` + `WeightedTrainer`.\n",
    "   * But turning `USE_CLASS_WEIGHTS = True` did **not** improve Kaggle score, sometimes slightly degraded it.\n",
    "   * Final choice: **disable class weights**.\n",
    "\n",
    "3. **Number of Epochs**\n",
    "\n",
    "   * Tried fewer epochs: 4-6 were my options.\n",
    "   * Settled on **6 epochs**; going much higher risked overfitting without clear performance gains.\n",
    "\n",
    "4. **Learning Rate Variants**\n",
    "\n",
    "   * Tried nearby values like 2e-5, 3e-5, 5e-5.\n",
    "   * Final best setting: **2e-5** on this checkpoint.\n",
    "\n",
    "5. **Preprocessing Variants**\n",
    "\n",
    "   * Lowercasing vs keeping original case:\n",
    "\n",
    "     * I **kept original case**, which empirically worked at least as well and avoids losing emphasis cues.\n",
    "   * More aggressive cleaning (removing emojis, removing all hashtags) generally did not help; merging hashtags into text and only removing URLs/mentions/HTML worked best.\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "1. **Pretrained Task-Specific Models Help**\n",
    "\n",
    "   Using a RoBERTa model already trained on emotion-related Twitter data (`cardiffnlp/twitter-roberta-base-emotion`) provided a strong starting point and outperformed generic base models in this competition.\n",
    "\n",
    "2. **Simple, Clean Preprocessing Is Sufficient**\n",
    "\n",
    "   The combination of:\n",
    "\n",
    "   * merging hashtags into the text,\n",
    "   * removing only URLs/mentions/HTML, and\n",
    "   * keeping emojis and case\n",
    "\n",
    "   turned out to be enough; more complex or aggressive preprocessing hurt performance.\n",
    "\n",
    "3. **Class Weights Are Not Always Beneficial**\n",
    "\n",
    "   Although the dataset is imbalanced, automatic class weighting did not improve the score for this particular model and dataset. Sometimes the pretrained model already encodes enough robustness, and additional weighting can destabilize fine-tuning.\n",
    "\n",
    "4. **Macro-F1 Is a Good Selection Metric**\n",
    "\n",
    "   Using `macro_f1` for `metric_for_best_model` focuses on performance across all 6 classes, not just the majority ones, and is more appropriate for emotion classification.\n",
    "\n",
    "5. **A Stable, Well-Tuned Baseline Can Beat More Complex Tricks**\n",
    "\n",
    "   Ultimately, the best result came from:\n",
    "\n",
    "   * a solid pretrained model,\n",
    "   * careful but not over-complicated preprocessing,\n",
    "   * a standard Trainer setup,\n",
    "   * and manual tuning of core hyperparameters (epochs, batch size, learning rate),\n",
    "\n",
    "   rather than from heavy-handed tricks like oversampling or custom loss functions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "%pip install -q transformers datasets evaluate accelerate\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import torch.nn as nn\n",
    "import evaluate\n",
    "\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"disabled\"\n",
    "\n",
    "DATA_DIR = \"/content\"\n",
    "OUT_DIR = \"/content\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Data loading\n",
    "# -----------------------------\n",
    "def load_data(data_dir):\n",
    "    with open(\"final_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    posts = pd.json_normalize(data).rename(\n",
    "        columns={\n",
    "            \"root._source.post.post_id\": \"post_id\",\n",
    "            \"root._source.post.text\": \"text\",\n",
    "            \"root._source.post.hashtags\": \"hashtags\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def merge_hashtags(row):\n",
    "        text = str(row[\"text\"])\n",
    "        if isinstance(row[\"hashtags\"], list) and len(row[\"hashtags\"]) > 0:\n",
    "            tags = \" \".join(row[\"hashtags\"])\n",
    "            return f\"{text} {tags}\"\n",
    "        return text\n",
    "\n",
    "    posts[\"text\"] = posts.apply(merge_hashtags, axis=1)\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    labels = pd.read_csv(\"emotion.csv\").rename(columns={\"id\": \"post_id\"})\n",
    "    split = pd.read_csv(\"data_identification.csv\").rename(columns={\"id\": \"post_id\"})\n",
    "\n",
    "    df = posts.merge(split, on=\"post_id\", how=\"left\").merge(labels, on=\"post_id\", how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Text preprocessing\n",
    "# -----------------------------\n",
    "URL_PATTERN = re.compile(r\"http\\S+|www\\S+\")\n",
    "MENTION_PATTERN = re.compile(r\"@\\w+\")\n",
    "HTML_TAG_PATTERN = re.compile(r\"<.*?>\")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # 保留大小寫以更好的辨認情緒\n",
    "    # text = str(text).lower()\n",
    "    # remove urls, mentions, html tags\n",
    "    text = URL_PATTERN.sub(\"\", text)\n",
    "    text = MENTION_PATTERN.sub(\"\", text)\n",
    "    text = HTML_TAG_PATTERN.sub(\"\", text)\n",
    "    # Let tokenizer handle the emojis/hashtags\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_clean_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"clean_text\"] = df[\"text\"].astype(str).apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "# -----------------------------\n",
    "# 3. Label mapping & split\n",
    "# -----------------------------\n",
    "LABEL_ORDER = [\"joy\", \"anger\", \"surprise\", \"sadness\", \"fear\", \"disgust\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(LABEL_ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "\n",
    "def prepare_splits(df: pd.DataFrame, test_size: float = 0.1, seed: int = 42):\n",
    "    df = df.copy()\n",
    "    df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "    df_test = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "    df_train[\"label_id\"] = df_train[\"emotion\"].map(LABEL2ID)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_train,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "        stratify=df_train[\"label_id\"],\n",
    "    )\n",
    "\n",
    "    train_df = train_df.rename(columns={\"label_id\": \"labels\"})\n",
    "    val_df = val_df.rename(columns={\"label_id\": \"labels\"})\n",
    "\n",
    "    return train_df, val_df, df_test\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Dataset & tokenization\n",
    "# -----------------------------\n",
    "def build_datasets(train_df, val_df, test_df, model_name=\"cardiffnlp/twitter-roberta-base-emotion\", max_len=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(\n",
    "        train_df[[\"clean_text\", \"labels\"]].reset_index(drop=True)\n",
    "    )\n",
    "    val_ds = Dataset.from_pandas(\n",
    "        val_df[[\"clean_text\", \"labels\"]].reset_index(drop=True)\n",
    "    )\n",
    "    test_ds = Dataset.from_pandas(\n",
    "        test_df[[\"clean_text\"]].reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"clean_text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "        )\n",
    "\n",
    "    train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "    val_ds = val_ds.map(tokenize_batch, batched=True)\n",
    "    test_ds = test_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "    def remove_cols(ds):\n",
    "        cols_to_remove = [c for c in [\"clean_text\", \"__index_level_0__\"] if c in ds.column_names]\n",
    "        if cols_to_remove:\n",
    "            ds = ds.remove_columns(cols_to_remove)\n",
    "        return ds\n",
    "\n",
    "    train_ds = remove_cols(train_ds)\n",
    "    val_ds = remove_cols(val_ds)\n",
    "    test_ds = remove_cols(test_ds)\n",
    "\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    return tokenizer, train_ds, val_ds, test_ds\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Class weights\n",
    "# -----------------------------\n",
    "def compute_class_weights(df_train: pd.DataFrame) -> torch.Tensor:\n",
    "    counts = df_train[\"labels\"].value_counts().sort_index().values\n",
    "    total = counts.sum()\n",
    "    num_classes = len(counts)\n",
    "    weights = total / (num_classes * counts)\n",
    "    return torch.tensor(weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "# -----------------------------\n",
    "# 6. Metrics\n",
    "# -----------------------------\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1_macro = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"macro_f1\": f1_macro}\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Weighted Trainer\n",
    "# -----------------------------\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 1. Load & preprocess\n",
    "print(\"Loading data ...\")\n",
    "df = load_data(DATA_DIR)\n",
    "df = add_clean_text(df)\n",
    "\n",
    "# 2. Train/val/test splits\n",
    "print(\"Preparing splits ...\")\n",
    "train_df, val_df, test_df = prepare_splits(df, test_size=0.1, seed=seed)\n",
    "\n",
    "# 3. Datasets & tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "max_len = 128\n",
    "tokenizer, train_ds, val_ds, test_ds = build_datasets(\n",
    "    train_df, val_df, test_df, model_name=model_name, max_len=max_len\n",
    ")\n",
    "\n",
    "# 4. Class weights\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    print(\"Computing class weights ...\")\n",
    "    class_weights = compute_class_weights(train_df)\n",
    "    print(\"Class weights:\", class_weights)\n",
    "else:\n",
    "    class_weights = None\n",
    "    print(\"Not using class weights.\")\n",
    "\n",
    "\n",
    "# 5. Model & training args\n",
    "print(\"Loading model ...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(LABEL_ORDER),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-emotion\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    ")\n",
    "\n",
    "if USE_CLASS_WEIGHTS and class_weights is not None:\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "else:\n",
    "    from transformers import Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "print(\"Start training ...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Predicting on test set ...\")\n",
    "pred_output = trainer.predict(test_ds)\n",
    "test_preds_id = pred_output.predictions.argmax(axis=-1)\n",
    "test_preds_label = [ID2LABEL[int(i)] for i in test_preds_id]\n",
    "\n",
    "print(\"Building submission file ...\")\n",
    "submission = df[df[\"split\"] == \"test\"][[\"post_id\"]].copy()\n",
    "submission[\"emotion\"] = test_preds_label\n",
    "out_path = os.path.join(OUT_DIR, \"submission_distilbert_weighted.csv\")\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(f\"Saved submission to {out_path}\")\n",
    "\n",
    "submission = submission.rename(columns={\"post_id\": \"id\"})\n",
    "out_path = os.path.join(OUT_DIR, \"submission_distilbert_weighted.csv\")\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(f\"Saved submission to {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dm2025lab)",
   "language": "python",
   "name": "dm2025lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
